{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzdYBBbg4QGZ"
   },
   "source": [
    "# Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aLslTbDJzxRm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch.nn import Module, Linear, ReLU, Softmax, Sequential, CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k80qMHan0c3i"
   },
   "source": [
    "# Define the XOR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qEzp5nQw0EAl"
   },
   "outputs": [],
   "source": [
    "# training set\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# true labels\n",
    "y = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShuZRSPc4N3p"
   },
   "source": [
    "# One-Hot Encoding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aRjpTMpg0gxE"
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(y: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "    \"\"\"Convert integer labels to one hot encoding.\n",
    "\n",
    "    Example: y=[1, 2], num_classes=3 --> [[0, 1, 0], [0, 0, 1]]\n",
    "\n",
    "    Args:\n",
    "        y: Input labels as integers with shape (num_datapoints)\n",
    "        num_classes: Number of possible classes\n",
    "\n",
    "    Returns:\n",
    "        One-hot encoded labels with shape (num_datapoints, num_classes)\n",
    "    \"\"\"\n",
    "    encoded = np.zeros(y.shape + (num_classes,))\n",
    "    encoded[np.arange(len(y)), y] = 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDDB6TO93_Al"
   },
   "source": [
    "# Define Neural Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xQ263clW0vyy"
   },
   "outputs": [],
   "source": [
    "def create_2unit_net() -> Module:\n",
    "    \"\"\"Create a two-layer MLP (1 hidden layer, 1 output layer) with 2 hidden units as described in the exercise.\n",
    "\n",
    "    Returns:\n",
    "        2-layer MLP module with 2 hidden units.\n",
    "    \"\"\"\n",
    "    # START TODO #################\n",
    "    # Define the model here\n",
    "    model = Sequential(\n",
    "        Linear(2,2),\n",
    "        ReLU(),\n",
    "        Linear(2,2)\n",
    "    )\n",
    "    # END TODO ##################\n",
    "    params = model.state_dict()\n",
    "\n",
    "    # Now we assign the model weights since we still did not learn backpropagation\n",
    "    params['0.weight'] = torch.Tensor(np.array([[3.21, 3.21], [-2.34, -2.34]]))\n",
    "    params['0.bias'] = torch.Tensor(np.array([-3.21, 2.34]))\n",
    "    params['2.weight'] = torch.Tensor(np.array([[3.19, 4.64], [-2.68, -3.44]]))\n",
    "    params['2.bias'] = torch.Tensor(np.array([-4.08, 4.42]))\n",
    "    model.load_state_dict(params)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BNIGIa6i0yvg"
   },
   "outputs": [],
   "source": [
    "def create_3unit_net() -> Module:\n",
    "    \"\"\"Create a two-layer MLP (1 hidden layer, 1 output layer) with 3 hidden units as described in the exercise.\n",
    "\n",
    "    Returns:\n",
    "        2-layer MLP module with 3 hidden units.\n",
    "    \"\"\"\n",
    "    # START TODO #################\n",
    "    # Define the model here\n",
    "    model = Sequential(\n",
    "        Linear(2,3),\n",
    "        ReLU(),\n",
    "        Linear(3,2)\n",
    "    )\n",
    "    # END TODO ##################\n",
    "\n",
    "    params = model.state_dict()\n",
    "    # START TODO #################\n",
    "    # change the model weights\n",
    "    params['0.weight'] = torch.Tensor(np.array([\n",
    "        [3.21, 3.21],  # Corresponds to the first hidden unit\n",
    "        [-2.34, -2.34],  # Corresponds to the second hidden unit\n",
    "        [0.0, 0.0]       # New hidden unit (neutral contribution)\n",
    "    ]))\n",
    "    params['0.bias'] = torch.Tensor(np.array([\n",
    "        -3.21,  # First hidden unit bias\n",
    "        2.34,   # Second hidden unit bias\n",
    "        0.0     # New hidden unit bias (neutral contribution)\n",
    "    ]))\n",
    "\n",
    "    # Assign weights and biases for the second layer\n",
    "    params['2.weight'] = torch.Tensor(np.array([\n",
    "        [3.19, 4.64, 0.0],  # First output unit weights\n",
    "        [-2.68, -3.44, 0.0] # Second output unit weights\n",
    "    ]))\n",
    "    params['2.bias'] = torch.Tensor(np.array([\n",
    "        -4.08,  # First output unit bias\n",
    "        4.42    # Second output unit bias\n",
    "    ]))\n",
    "\n",
    "    # END TODO ##################\n",
    "    model.load_state_dict(params)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHAZY3ze4Gux"
   },
   "source": [
    "#Run the Model on XOR Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pprNr8FY3FFo"
   },
   "outputs": [],
   "source": [
    "def run_model_on_xor(model: Module, verbose: bool = True) -> Tuple[np.ndarray, np.float32]:\n",
    "    \"\"\"Run the XOR dataset through the model and compute the loss.\n",
    "\n",
    "    Args:\n",
    "        model: MLP to use for prediction\n",
    "        verbose: Whether to print the outputs.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            Class predictions after softmax with shape (batch_size, num_classes)\n",
    "            Cross-Entropy loss given the model outputs and the true labels\n",
    "\n",
    "    \"\"\"\n",
    "    # Here we test if our prediction works. We first get the so-called \"logits\" (the MLP output before the softmax),\n",
    "    # then run them through the softmax function. We have to transform the prediction into one-hot format,\n",
    "    # and finally we can check whether our MLP predicts the correct values.\n",
    "\n",
    "    # START TODO #################\n",
    "    # propagate the input data (stored in the imported variable X) through the model.\n",
    "    prediction = model(torch.Tensor(X))\n",
    "    # END TODO ##################\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Raw prediction logits:\")\n",
    "        print(prediction.detach().cpu().numpy())\n",
    "        print()\n",
    "    softmax_function = Softmax(dim=-1)\n",
    "    pred_softmax = softmax_function(prediction).cpu().detach().numpy()\n",
    "    if verbose:\n",
    "        print(\"Prediction after softmax:\")\n",
    "        print(pred_softmax)\n",
    "        print()\n",
    "\n",
    "    # START TODO #################\n",
    "    # Use the one_hot_encoding function on the labels to convert them to\n",
    "    # one-hot encoding. The labels are stored in the imported variable y.\n",
    "    Y_onehot = one_hot_encoding(y, 2)\n",
    "    # END TODO ##################\n",
    "\n",
    "    if verbose:\n",
    "        print(\"True labels, one-hot encoded:\")\n",
    "        print(Y_onehot)\n",
    "        print()\n",
    "\n",
    "    # Pass prediction and ground truth to the generalized Cross-Entropy Loss.\n",
    "    # Since the loss has a softmax already implemented inside of it, you need to pass the raw logits of the\n",
    "    # prediction. The loss expects one-hot encoded labels of shape (batchsize, num_classes)\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    # given the true labels Y and the predictions,\n",
    "    # compute the cross entropy loss defined above\n",
    "    loss = loss_fn(prediction, torch.LongTensor(y)).cpu().detach().numpy()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Loss:\", loss)\n",
    "\n",
    "    # return predictions and loss for testing\n",
    "    return pred_softmax, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7g965ODo3Hop"
   },
   "outputs": [],
   "source": [
    "def run_test_model(model: Module) -> None:\n",
    "    \"\"\"Helper function to test if the model predicts the correct classes.\n",
    "\n",
    "    Args:\n",
    "        model: Module to predict the classes.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    pred_softmax, loss = run_model_on_xor(model, verbose=False)\n",
    "    Y_onehot = one_hot_encoding(y, 2)\n",
    "    np.testing.assert_allclose(\n",
    "        pred_softmax, Y_onehot, atol=1e-3,\n",
    "        err_msg=f\"The model predicts the wrong classes. Ground-truth: {Y_onehot}, predictions: {pred_softmax}\")\n",
    "    assert np.abs(loss) < 1e-3, f\"Loss is too high: {loss}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jPRQSpPc8GVD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw prediction logits:\n",
      "[[ 6.7775993 -3.6295996]\n",
      " [-4.08       4.42     ]\n",
      " [-4.08       4.42     ]\n",
      " [ 6.1599007 -4.1828003]]\n",
      "\n",
      "Prediction after softmax:\n",
      "[[9.9996984e-01 3.0213278e-05]\n",
      " [2.0342699e-04 9.9979657e-01]\n",
      " [2.0342699e-04 9.9979657e-01]\n",
      " [9.9996781e-01 3.2226126e-05]]\n",
      "\n",
      "True labels, one-hot encoded:\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "\n",
      "Loss: 0.00011732114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[9.9996984e-01, 3.0213278e-05],\n",
       "        [2.0342699e-04, 9.9979657e-01],\n",
       "        [2.0342699e-04, 9.9979657e-01],\n",
       "        [9.9996781e-01, 3.2226126e-05]], dtype=float32),\n",
       " array(0.00011732, dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_2unit_net()\n",
    "run_model_on_xor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Xswxf9jo8J6R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw prediction logits:\n",
      "[[ 6.7775993 -3.6295996]\n",
      " [-4.08       4.42     ]\n",
      " [-4.08       4.42     ]\n",
      " [ 6.1599007 -4.1828003]]\n",
      "\n",
      "Prediction after softmax:\n",
      "[[9.9996984e-01 3.0213278e-05]\n",
      " [2.0342699e-04 9.9979657e-01]\n",
      " [2.0342699e-04 9.9979657e-01]\n",
      " [9.9996781e-01 3.2226126e-05]]\n",
      "\n",
      "True labels, one-hot encoded:\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "\n",
      "Loss: 0.00011732114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[9.9996984e-01, 3.0213278e-05],\n",
       "        [2.0342699e-04, 9.9979657e-01],\n",
       "        [2.0342699e-04, 9.9979657e-01],\n",
       "        [9.9996781e-01, 3.2226126e-05]], dtype=float32),\n",
       " array(0.00011732, dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_3unit_net()\n",
    "run_model_on_xor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
